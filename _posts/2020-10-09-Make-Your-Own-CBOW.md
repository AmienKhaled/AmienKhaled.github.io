---
title: "ما هو ال CBOW وكيفية تطويره من الصفر وتطبيقه على اللغة العربية"
tagline: "في هذا المقال سنتعرف أكثر على طريقة CBOW لصناعة Word2Vec وتطبيقها على اللغة العربية"
excerpt: "في هذا المقال سنتعرف أكثر على طريقة CBOW لصناعة Word2Vec وتطبيقها على اللغة العربية"
header:
  overlay_image: images/2020-10-09-Make-Your-Own-CBOW/header.png
  teaser: images/2020-10-09-Make-Your-Own-CBOW/header.png
categories:
  - Blog
tags:
    - Word2vec
    - NLP
    - Python
    - CBOW
toc: true
toc_sticky: true
---
<style>
.wrapper, .toc__menu, table, .ara{
  direction: rtl;
}
p {
  text-align: justify;
  text-justify: inter-word;
}
</style>
 
 
## مقدمة
{: .text-right}
 
<p dir='rtl'>
في هذا المقال سنتعرف سويا على موضوع مهم في علم معالجة اللغات الطبيعية (NLP) وهو Word embeddings وكيفية تطبيقة باستخدام طريقة ال CBOW  والتي سنبرمجها معا من الصفر باستخدام  لغة البرمجة بايثون (Python)
</p>
 
 
## بعض المتطلبات المسبقة
{: .text-right}

<p dir='rtl'>
قبل ان نبدا معاً يجب ان نعرض بعض المتطلبات المسبقة لفهم المفال بشكل سهل وسلسل
</p>
 
- **Python**
- **Deep Learning**
- **Gradient descent**
 
## ترجمة بعض المصطلحات العلمية
{: .text-right}

<p dir='rtl'>
خلال هذا المقال سوف نستخدم المصطلحات العلمية بلغتها الاصلية حيث ان ترجمتها سيضع بعض الغموض و سيصعب عليك البحث والتعلم في هذا الموضوع فيما بعد لذلك سأكتبها كما هي وسأضع هنا بعض الترجمات التي يمكن العثور عليها 
</p>
 
- **Vector** : مُتجه
- **Word2Vec** : تمثيل الكلمات لمُتجهات
- **Continuous Bag of words (CBOW)** : واحدة من الطرق المستخدمة لتمثيل الكلمات كمتجهات
- **Natural language processing (NLP)** : معالجة اللغات الطبيعية
- **Gradient descent** : هي الخوارزمية المستخدمة لتحسين الخطأ الناتج من التعلم قدر الإمكان
- **Deep Learning** : التعلم العميق، وهو فرع من فروع علم الذكاء الاصطناعي
- **Model**:
- **Integers**: عدد صحيح
 
## مشكلة تمثيل الكلمات 
{: .text-right}

<p dir='rtl'>
في ال <strong>Deep learning</strong> يجب أن تكون البيانات المستخدمة في التعلم على هيئة أرقام ،فعلى سبيل اذا اردت بناء model للتعرف على مدى رضا العملاء عن خدمة معينة من خلال تعليقاتهم، فستحتاج بلا شك لبيانات  مكونة من كلمات (نصوص التعليقات المكتوب بواسطة العملاء) ، ولكي تبدأ عملية التعلم فعليك تحويل تلك النصوص لأرقام حتى يستطيع ال model التعامل معها، وهنا سيظهر سؤال كيف يمكن تمثيل النصوص لأرقام؟
</p>

## الطرق المختلفة لتمثيل النصوص الي ارقام
{: .text-right}

<p dir='rtl'>
هناك العديد من الطرق لتميل النصوص سنعرض هنا بض منها ، وسنطبق في النهاية بطريقة ال CBOW
</p>

## Integers
{: .text-right}
<p dir='rtl'>
في هذة الطريقة نستخرج الكلمات من النصوص المتوافرة لدينا بدون تكرار،وبعذ ذلك نضع لها رقم صحيح مميز لها . على سبيل المثال لنتخيل ان النصوص الاتية هي البيانات والمطلوب تمثيلها بطريقة الاعداد الصحيحة
</p>

<div dir='rtl' class='notice--success'>
1-   أنا أحب تعلم البرمجة
<br>
2- البرمجة تفتح أفق جديدة للعالم
<br>
3- أنا أحب تطوير تطبيقات جديدة
<br>
<br>
</div>

<p dir='rtl'>
الكلمات الموجودة في المثال بدون تكرار :
</p>

<div dir='rtl' class='notice--success'>
أنا-أحب-تعلم-البرمجة-تفتح-أفق-جديدة-للعالم-تطوير-تطبيقات   
</div>

| الكلمة      | الرقم |
| ----------- | ----------- |
| أنا      | 1       |
| أحب   | 2        |
| تعلم   | 3        |
| البرمجة   | 4        |
| تفتح   | 5        |
| أفق   | 6        |
| جديدة   | 7        |
| للعالم   | 8        |
| تطوير   | 9        |
| تطبيقات   | 10        |

<p dir='rtl'>
قد تكون هذة الطريقة حلت مشكلت تمثيل البيانات ولكن تظل هناك مشكلة حقيقية في ان الارقام   التي تمثل الكلمات لاتحمل اي معني، فيمكن تبديل الارقم المقابلة للكمات باي رقم اخر ،كمثال يمكن التبديل بين كلمتين (أنا:1 ، تعلم:3) في الجدول بالاعلى لتصبح (انا :3 ، تعلم:1)
</p>


## One-hot Vectors
{: .text-right}
<p dir='rtl'>
فهذة الطريقة تاخد كل كلمة رقم خاص بها ، ولكن هنا يتم تمثيل الكلمة من خلال متجه (Vector)  جميع عناصره قمها صفر ماعدا العنصر في المكان القابل لرقم الكلمة فيكون قيمته بصفر ، شكل هذا المتجه $$M*1$$ حيث M عدد الكلمات في النص بدون تكرار. ليكون الامر اوضح دعونا نطبق على المثال بالاعلى تلك الطريقة
</p>

<div dir='rtl' class='notice--success'>
في المثال بالاعلى كلمة "أفق" تحمل الرقم 6 ،سنلاحظ هنا ان المتجه الخاص بكلمة "أفق" جميع عناصره قيمتها صفر ماعاد العنصر رقم 6 ،سنلاحظ ايضا ان هذا المتجه خاص فقط بهذه الكلمة
</div>

|        | أفق |
| ----------- | ----------- |
| 1      | 0       |
| 2   | 0        |
| 3   | 0        |
| 4   | 0        |
| 5   | 0        |
| 6   | 1        |
| 7   | 0        |
| 8   | 0        |
| 9   | 0        |
| 10   | 0        |

<p dir='rtl'>
مميزات تلك الطريقة
</p>
- بسيطة لاتحمل تعقيد لبرمجتها
{: .ara}
<p dir='rtl'>
عيوب تلك الطريقة
</p>
- حجم المتجاهات يمكن ان يكون ضخم فنتخيل مثلا عدد الكلمات 12M 
{: .ara}
- المتجهات في هذة الحاله لاتحمل ايضا معني خاص للكلمة يعبر عنها
{: .ara}


## Word Embeddings
{: .text-right}
<p dir='rtl'>
لتبسيط هذا المفهوم ، تخيل ان هناك خط يزيد فيه المعنى الايجابي كلما اتجهنا يمينا، ويزيد فيه المعنى السلبي كلما اتجهنا لليسار كما هو موضح بالاسفل . فبهذة الطريقة  يمكننا تمثيل الكلمة من خلال متجه ذات بعد واحد 1*1 يعبر عن مدي ايجابية او سلبية الكلمة فعلى سبيل المثال اذا اخترنا رقم عشوائي (مثلاً 3) فبالتأكيد سيكون هذا الرقم تمثيل لكلمة إيجابية وعلى العكس إذا اخترنا رقم اقل من الصفر.
</p>

![words line](/images/2020-10-09-Make-Your-Own-CBOW/words_line.png)

<p dir='rtl'>
بنفس الطريقة تخيل ان هناك بدل الخط، محورين محور راسي ومحور افقي كما هو موضح بالاسفل،بحيث تكون الكلمة تعبر عن شئ مادي كلما اتجهنا لاعلى، وتعبر الكلمة عن شئ اكثر معنوية كلما اتجهنا لاسفل. مكننا التعبير عن الكلمة في هذة الحالة بمتجه بشكل 2*1 ، كمثال يمكنن تمثيل كلمة كلب بالمتجه [1.09, 0.57] . في هذهة الحالة اصبح المتجه يعبر بشكل اكبر عن الكلمة ، فكلما زادت قيمة المحور الافقي كلما ذادت المني الايجابي للكلمة ،وكلما زادت قيمت المحور الرأسي اصبحت الكلمة اكثر مادية، وهكذا في باقي الاتجاهات. فبذلك اصبح التثيل يعبر بقوة اكثر عن الكلمة. وكلما زادت المحاور او كما نسميها الابعاد التي تمثل الكلمة كلما اصبح هذا التمثيل يعبر بقوة اكثر عن الكلمة.
</p>


![words line](/images/2020-10-09-Make-Your-Own-CBOW/words_2axis.png)

<p dir='rtl'>
مميزات تلك الطريقة
</p>

- أصبحت المتجهات في هذة الطريقة حجمها اقل فعادة تكون بين 100~1000 ولكن في الطريقة السابقة ربما تتجاوز المليون 
{: .ara}

- أصبح المتجه يحمل معني لكل كلمة 
  -  اي المسافة بين الكلمات القريبة في المعني اقصر
    - مثال : متجه كلمة "شجرة" تقريبا يساوى المتجه الممثل لكلمة "أشجار" ويختلف تماماً عن المتجه الممثل لكلمة "تذكرة"
{: .ara}
  - تستطيع استخراج التشابه الجزئي بين الكلمات
    - مثال: كلمة "باريس" بالنسبة لكلمة "فرنسا"  مثل كلمة "القاهرة بالنسبة لكلمة ؟ فاذا كنا نمتلك Word Embeddings صحيحة فسيكون الناتج كلمة "مصر"
{: .ara}

<p dir='rtl'>
والان بعد ان تعرفنا على الفكرة الاساسية التي تعتمد عليها ال word embeddings 
كيف يمكننا حسابها ؟ فبالطبع لن نرسم محاور ونضع كل كلمة على تلك المحاور ، وهنا ياتي دول الDeep learning ، سنتعرف في الفقرات القادمة على الشهر الطرق المستخدمة لحساب تلك المتجهات
</p>

## الطرق المختلفة حصول على ال Word Embeddings
{: .text-right}
<p dir='rtl'>
والان بعد ان تعرفنا على الفكرة الاساسية التي تعتمد عليها ال word embeddings 
كيف يمكننا حسابها ؟ فبالطبع لن نرسم محاور ونضع كل كلمة على تلك المحاور ، وهنا ياتي دول الDeep learning ، سنتعرف في الفقرات القادمة على الشهر الطرق المستخدمة لحساب تلك المتجهات
</p>

## ١- طرق بسيطة 
{: .text-right}
<p dir='rtl'>
المشترك في هذة الطرق الثلاثة ان كل كلمة لها متجه واحد فقط بغض النظر عن موقع الكلمة في الجملة (كمثال: رأيت عين ما)   
</p>

<div dir='rtl' class='notice--success'>
١- رأيت عين ماء في الصحراء
<br>
٢- يوجد للذبابة أكثر من عين
<br>
</div>

<p dir='rtl'>
معنى كلمة "عين" في الجملة الاولى تختلف عن معني "عين" في الجملة الثانية ولكن لهما نفس قيمة المتجه  
</p>

- Word2Vec (Google, 2013)
  - Continuous bag-of-word (CBOW)
  - Continuous Skip-gram/Skip-gram with negative sampling (SGNS)

- Global Vectors (GLoVe) (Stanford, 2014)

- fastText (Facebook, 2016)
  - المميز في هذهة الطريقة انها تستطيع حساب متجهات لكلمات لم تراها من قبل لانها تعمل على الحروف بدلاً عن الكلمات


## ٢- طرق أكثر تعقيداً
{: .text-right}
<p dir='rtl'>
المشترك في هذهة الطرق الثلاثة ان حساب المتجه للكلمة متغير بحسب معناها في الجملة، فإذا طبقنا على الثال السابق ، سيصبح لكلمة "عين" في الجملتين متجهين مختلفين 
</p>

- BERT (Goole, 2018)
- ELMo (Allen Institute for AI, 2018)
- GPT-2 (OpenAI, 2018)

## ما هي طريقة ال CBOW 
{: .text-right}
<p dir='rtl'>
CBOW هو طريقة نستطيع بها حساب ال Word emmbeddings ، وتعتمد هذة الطريقة على  تدريب model للتنبؤ بالكلمة من مجموعة الكلمات المحيطة بتلك الكلمة. مثال : 
</p>

<div dir='rtl' class='notice--success'>
رأيت اليوم __؟__ صغير ينبح.
</div>

<p dir='rtl'>
الكلمات المتاحة لل model للتنبؤ :
</p>

<div dir='rtl' class='notice--success'>
[كلب - قطة - ورقة - ملعب]
</div>
<p dir='rtl'>
قيجب ان يختار ال model هنا كلمة كلب 
</p>

## كيفية تحضير الامثلة وال dataset
{: .text-right}
<p dir='rtl'>
قبل البدء في شرح هذا بالتفصيل ، هناك بعض المفاهيم التي يجب معرفتها
</p>

<p dir='rtl'>
تسمى الكلمة المراد التنبؤ بها ب (center word)، وتسمى الكلمات المحيطة بالكلمة من اليمين و اليسار بال (Context Word) ، عدد الكلمات المستخدم على  اي جانب من بال (context half-size)، ويرمز لها بالرمز (C) ، ويطلق على مجموع (Cotext word + center word) بال (windows) ويتم حسابه باستخدام هذة المعادلة $$2*c + 1$$
مثال :
</p>

<div dir='rtl' class='notice--success'>
كما هو موضح بالصورة ال (context half-size) يساوي 2، لحساب ال (windows size)، نستخدم المعادلة  بالاعلى   
$$window size=2*5+1=5$$
</div>

![words line](/images/2020-10-09-Make-Your-Own-CBOW/cbow_ele.png)


## شكل ال Dataset
{: .text-right}
<p dir='rtl'>
لنستطيع تطبيق طريقة ال CBOW يجب ان تكون النصوص بشكل معين، وهو ان تكون ال (Context words) هي المدخلات لل Model وتكون ال المخرجات او ال labels هي ال (Ccenter word) ، فلنفترض ان النصوص   بالاسفل سنستخدمها في عملية التدريب، وسنستخدم (context half-sizr) يساوي 1 
</p>

<div dir='rtl' class='notice--success'>
القراءة عملية معرفية تستند على تفكيك رموز تسمى حروفا لتكوين معنى، والوصول إلى مرحلة الفهم والإدراك.
</div>


<p dir='rtl'>
فستكون ال Dataset بهذا الشكل بالاسفل ، حيث سيرمز للمدخلات بالرمز X  والمخرجات بالرمز Y
</p>



| X      | Y |
| ----------- | ----------- |
| القراءة معرفية      | عملية       |
| عملية تستند   | معرفية        |
| معرفية على   | تستند        |
| تستند تفكيك   | على        |
| على رموز   | تفكيك        |
| تفكيك تسمى   | رموز        |
| رموز حروفا   | تسمى        |
| تسمى لتكوين   | حروفا        |
| حروفا معنى   | لتكوين        |
| لتكوين والوصول   | معنى        |
| معنى إالى   | والوصول        |
| والوصول مرحلة   | إالى        |
| إالى الفهم   | مرحلة        |
| مرحلة والإدراك   | الفهم        |
| الفهم .   | والإدراك        |







